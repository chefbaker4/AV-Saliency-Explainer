<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Saliency Based Natural Language Explanations for Autonomous Vehicles</title>
  <meta name="description" content="Project page for AV saliency-grounded natural language explanations." />

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">

  <!-- Styles -->
  <style>
    :root {
      --bg: #0b0f19;
      --card: #121828;
      --muted: #aab3c5;
      --text: #ecf1ff;
      --accent: #66b2ff;
      --accent-2: #9b8cff;
      --radius: 18px;
      --shadow: 0 10px 30px rgba(0,0,0,.35);
    }

    * { box-sizing: border-box; }

    html, body {
      margin: 0;
      padding: 0;
      background: radial-gradient(1200px 600px at 20% -10%, rgba(102,178,255,.15), transparent 60%),
                  radial-gradient(1200px 600px at 100% 20%, rgba(155,140,255,.12), transparent 60%),
                  var(--bg);
      color: var(--text);
      font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
    }

    a { color: var(--accent); text-decoration: none; }

    .toggleBtn {
      background-color: #007bff;
      color: white;
      border: none;
      padding: 12px 24px;
      font-size: 18px;
      border-radius: 8px;
      cursor: pointer;
    }
    .toggleBtn:hover {
      background-color: #0056b3;
    }

    .container {
      max-width: 1100px;
      margin: 0 auto;
      padding: 40px 20px 80px;
    }

    /* Hero */
    .hero {
      background: linear-gradient(135deg, rgba(102,178,255,.25), rgba(155,140,255,.25));
      border: 1px solid rgba(255,255,255,.08);
      box-shadow: var(--shadow);
      border-radius: var(--radius);
      padding: 28px;
      margin: 24px 0 28px;
    }
    h1 {
      margin: 0 0 10px;
      font-weight: 800;
      letter-spacing: .3px;
    }
    .authors {
      color: var(--muted);
      font-weight: 600;
      margin-top: 6px;
    }
    .badge {
      display: inline-block;
      padding: 4px 10px;
      border-radius: 12px;
      background: rgba(102,178,255,.15);
      color: #d8e9ff;
      border: 1px solid rgba(102,178,255,.35);
      font-size: 12px;
      font-weight: 700;
      letter-spacing: .3px;
      text-transform: uppercase;
    }

    /* Section */
    .section {
      background: var(--card);
      border: 1px solid rgba(255,255,255,.06);
      border-radius: var(--radius);
      padding: 28px;
      margin: 22px 0;
      box-shadow: var(--shadow);
    }
    .section h2 {
      margin-top: 0;
      letter-spacing: .2px;
    }
    .muted { color: var(--muted); }

    /* Media blocks */
    .grid {
      display: grid;
      grid-template-columns: 1fr;
      gap: 18px;
    }
    @media (min-width: 850px) {
      .grid-2 { grid-template-columns: 1fr 1fr; }
    }

    .video-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      max-width: 1600px;
      margin: 0 auto;
    }
    figure {
      margin: 0;
      background: rgba(255,255,255,.02);
      border: 1px solid rgba(255,255,255,.05);
      border-radius: 14px;
      overflow: hidden;
    }
    figure figcaption {
      font-size: 16px;
      padding: 8px 12px 12px;
      color: var(--muted);
      border-top: 1px solid rgba(255,255,255,.06);
    }
    .caption-title {
      display: block;
      font-weight: 600;
      color: var(--text);
      margin-bottom: 6px;
    }
    video {
      display: block;
      width: 100%;
      height: auto;
      background: #0d111a;
    }


    /* Captions */
    .two-captions {
      display: grid;
      grid-template-columns: 1fr;
      gap: 8px;
    }
    @media (min-width:600px) {
      .two-captions { grid-template-columns: 1fr 1fr; }
    }

    /* Footer */
    .footer {
      text-align: center;
      color: var(--muted);
      margin-top: 26px;
      font-size: 12px;
    }
    .motivating-example {
      text-align: center;
      margin: 2rem 0;
    }

    .motivating-example img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;  /* optional rounded corners */
      box-shadow: 0 4px 12px rgba(0,0,0,0.15); /* optional shadow */
    }

    .motivating-example figcaption {
      font-size: 0.9rem;
      color: var(--text);
      margin-top: 0.5rem;
    }

    .flowchart {
      text-align: center;
      margin: 2rem 0;
    }

    .flowchart img {
      display: block;
      margin: 0 auto;
      max-width: 60%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }

    .flowchart figcaption {
      font-size: 0.9rem;
      color: #666;
      margin-top: 0.5rem;
    }

    
  </style>
</head>
<body>
  <main class="container">

    <!-- Hero -->
    <header class="hero">
      <div class="badge">Project Page</div>
      <h1>Saliency Based Natural Language Explanations for Autonomous Vehicles</h1>
      <div class="authors">Daniel Baker · Adib Rouf · Guo Zhefan (Ben)</div>
    </header>

    <!-- Motivating Example Image -->
    <figure class="motivating-example">
      <h2 class="figure-title">A motivating example</h3>
      <img src="figures/motivating_example.png" alt="Motivating Example">
      <figcaption>Saliency maps are computed using the internal calculations of an autonomous vehicle and reveal what regions the model focused on when making a decision. Our method uses this to to faithfully caption the vehicle's behavior and reasoning. </figcaption>
    </figure>


    <!-- Overview -->
    <section class="section" id="overview">
      <h2>Overview</h2>
      <p class="muted">
        We aim to provide natural language explanations of Autonomous Vehicle (AV) behavior to build user trust and encourage adoption of this technology. Today, passengers may be left wondering why their AV makes certain decisions—such as stopping abruptly—without any clear explanation. Our approach addresses this gap by revealing the vehicle’s motivations in plain English.
      </p>
      <p class="muted">
        Modern end-to-end AV systems achieve impressive performance but remain opaque “black boxes,” raising trust and safety concerns. Explainable AI (XAI) techniques like saliency maps consider a model's internal calculations and highlight the input regions that influence the AV's decisions. We propose a vision language model (VLM) framework that incorporates these saliency maps into caption generation, producing faithful, region-specific explanations of AV behavior.
      </p>
      <p class="muted">
        Current VLMs have no mechanism to incorporate saliency maps, and thus cannot truly reveal an AV's thought process. Rather, they simply act as a video captioning tool. We refer to them as saliency-agnostic models and say they produce video captions. On the other hand, our model is saliency-aware and produces vehicular captions.
      </p>
    </section>


    <!-- flowchart -->
    <figure class="flowchart">
      <img src="figures/flowchart.png" alt="flowchart">
      <!-- <figcaption>Motivating example of our method</figcaption> -->
    </figure>

    <!-- Video Analysis 1-->
    <section class="section" id="video-analysis">
      <h2>Model Interpretation</h2>
      <h3>Exmple 1</h3>
      <p class="muted">
        This side-by-side comparison shows our model with and without saliency maps and demonstrates how incorporating saliency maps results in more faithful descriptions. For both of these examples, we first generate captions without slaiency maps to serve as a baseline. We see the captions are plausible. However, when introducing a hypothetical saliency map representing the focus of an AV, we see that the model may have truly had different reasoning. Consider the first video where the truck pulls ahead and the pedestrian crosses the street. A heat map reveals that the AV was not waiting for the light to change but actually stopped because of the pedestrian. We levearge this valuable information and provide a caption that relfects this reasoning.
      </p>
      
      <div class="video-grid">
        <figure>
          <video id="video1" src="demo_videos/pedestrian_raw_big.mp4" playsinline></video>
          <figcaption>
            <span class="caption-title">Predicted Caption (Saliency Agnostic)</span>
            The car is stopped because the driver is waiting for the light to change.
          </figcaption>
        </figure>

        <figure>
          <video id="video2" src="demo_videos/pedestrian_overlay_big.mp4" playsinline></video>
          <figcaption>
            <span class="caption-title">Predicted Caption (Saliency Aware)</span>
            The car is stopped because the driver is waiting for a pedestrian to cross the street.
          </figcaption>
        </figure>
      </div>

      <div style="text-align:center; margin-top:1rem;">
        <button id="toggleBtn1" class="toggleBtn">Play</button>

      </div>

      <script>
      const v1 = document.getElementById("video1");
      const v2 = document.getElementById("video2");
      const toggleBtn1 = document.getElementById("toggleBtn1");

      toggleBtn1.addEventListener("click", () => {
        if (v1.paused || v2.paused) {
          v1.currentTime = 0;
          v2.currentTime = 0;
          v1.play();
          v2.play();
          toggleBtn1.textContent = "Pause";
        } else {
          v1.pause();
          v2.pause();
          toggleBtn1.textContent = "Play";
        }
      });
      </script>



      <div style="height: 25px;"></div>
    <!-- Video Analysis 2-->

      <h3>Exmple 2</h3>
      <p class="muted"> In this next example we have a similar case where the raw caption may cite that the model is primarily driving due to the road being clear, when in reality it is driving through many intersections because the light is also green.</p>
      
      <div class="video-grid">
        <figure>
          <video id="video3" src="demo_videos/traffic_lights_raw_big.mp4" playsinline></video>
          <figcaption>
            <span class="caption-title">Predicted Caption (Saliency Agnostic)</span>
            The car is driving forward because the road is clear.
          </figcaption>
        </figure>

        <figure>
          <video id="video4" src="demo_videos/traffic_lights_overlay_big.mp4" playsinline></video>
          <figcaption>
            <span class="caption-title">Predicted Caption (Saliency Aware)</span>
            The car is driving forward because the light is green.
          </figcaption>
        </figure>
      </div>

      <div style="text-align:center; margin-top:1rem;">
        <button id="toggleBtn2" class="toggleBtn">Play</button>

      </div>

      <script>
      const v3 = document.getElementById("video3");
      const v4 = document.getElementById("video4");
      const toggleBtn2 = document.getElementById("toggleBtn2");

      toggleBtn2.addEventListener("click", () => {
        if (v3.paused || v4.paused) {
          v3.currentTime = 0;
          v4.currentTime = 0;
          v3.play();
          v4.play();
          toggleBtn2.textContent = "Pause";
        } else {
          v3.pause();
          v4.pause();
          toggleBtn2.textContent = "Play";
        }
      });
      </script>

      <div style="height: 25px;"></div>

    <!-- Video Analysis 3-->
      <h3>Exmple 3</h3>
      <p class="muted">
        Here is another example where the saliency-agnostic model assumed the car stopped for one reason, but the saliency map revealed the AV actually stopped because of the pedestrian. Our saliency-aware caption reflects this!
      </p>
      
      <div class="video-grid">
        <figure>
          <video id="video5" src="demo_videos/three_things_raw_trimmed.mp4" playsinline></video>
          <figcaption>
            <span class="caption-title">Predicted Caption (Saliency Agnostic)</span>
            The car stopped at the red light.
          </figcaption>
        </figure>

        <figure>
          <video id="video6" src="demo_videos/three_overlay.mp4" playsinline></video>
          <figcaption>
            <span class="caption-title">Predicted Caption (Saliency Aware)</span>
            The car stopped because the pedestrian was crossing the street.
          </figcaption>
        </figure>
      </div>

      <div style="text-align:center; margin-top:1rem;">
        <button id="toggleBtn3" class="toggleBtn">Play</button>

      </div>

      <script>
      const v5 = document.getElementById("video5");
      const v6 = document.getElementById("video6");
      const toggleBtn3 = document.getElementById("toggleBtn3");

      toggleBtn3.addEventListener("click", () => {
        if (v5.paused || v6.paused) {
          v5.currentTime = 0;
          v6.currentTime = 0;
          v5.play();
          v6.play();
          toggleBtn3.textContent = "Pause";
        } else {
          v5.pause();
          v6.pause();
          toggleBtn3.textContent = "Play";
        }
      });
      </script>

      <div style="height: 25px;"></div>

    <!-- Video Analysis 4-->
    <h3>Exmple 4</h3>
      <p class="muted">
        Finally, we have another example where our model helps reveal the AV's true intentions with natural language.
      </p>
      
      <div class="video-grid">
        <figure>
          <video id="video7" src="demo_videos/multispot_raw_trimmed.mp4" playsinline></video>
          <figcaption>
            <span class="caption-title">Predicted Caption (Saliency Agnostic)</span>
            The car is turning right because the light is green.
          </figcaption>
        </figure>

        <figure>
          <video id="video8" src="demo_videos/multispot_overlay.mp4" playsinline></video>
          <figcaption>
            <span class="caption-title">Predicted Caption (Saliency Aware)</span>
            The car is moving forward slowly because the pedestrian is crossing the street.
          </figcaption>
        </figure>
      </div>

      <div style="text-align:center; margin-top:1rem;">
        <button id="toggleBtn4" class="toggleBtn">Play</button>

      </div>

      <script>
      const v7 = document.getElementById("video7");
      const v8 = document.getElementById("video8");
      const toggleBtn4 = document.getElementById("toggleBtn4");

      toggleBtn4.addEventListener("click", () => {
        if (v7.paused || v8.paused) {
          v7.currentTime = 0;
          v8.currentTime = 0;
          v7.play();
          v8.play();
          toggleBtn4.textContent = "Pause";
        } else {
          v7.pause();
          v8.pause();
          toggleBtn4.textContent = "Play";
        }
      });
      </script>

    </section>

    <!-- Model Demonstration -->
    <section class="section" id="model-demo">
      <h2>Web Page Demonstration</h2>
      <p class="muted">Our application is built on a full-stack pipeline that connects a React frontend, a Flask backend, and Supabase for storage and database management. The React client sends requests to the Flask server, which handles the backend logic in Python and interacts with Supabase. When a user uploads a video, it is stored in a Supabase storage bucket and tracked in the database. The backend model accesses this file, processes it on the GPU, and writes the output back into the database. The frontend then retrieves this response and displays it seamlessly on the user’s screen, creating a smooth flow between user interaction, backend computation, and cloud data services.</p>
      <figure>
        <video class="video-small" src="demo_videos/gui_demo.mp4" controls playsinline></video>
      </figure>
    </section>

    <!-- Footer -->
    <div class="footer">© UConn for now...</div>

  </main>
</body>
</html>
